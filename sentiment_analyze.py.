import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


class SentimentAnalyzer:
    def __init__(self, max_words=10000, max_sequence_length=100):
        self.max_words = max_words
        self.max_sequence_length = max_sequence_length
        self.tokenizer = Tokenizer(num_words=self.max_words)
        self.model = self._build_model()
        self.label_encoder = LabelEncoder()

    def _build_model(self):
        model = Sequential()
        model.add(Embedding(self.max_words, 128, input_length=self.max_sequence_length))
        model.add(SpatialDropout1D(0.2))
        model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
        model.add(Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model

    def fit(self, texts, labels, epochs=5, batch_size=64, validation_split=0.2):
        # Fit the tokenizer on the texts
        self.tokenizer.fit_on_texts(texts)

        # Convert texts to sequences and pad them
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded_sequences = pad_sequences(sequences, maxlen=self.max_sequence_length)

        # Encode labels
        encoded_labels = self.label_encoder.fit_transform(labels)

        # Train the model
        history = self.model.fit(
            padded_sequences,
            encoded_labels,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            verbose=1
        )
        return history

    def predict_sentiment(self, reviews):
        sequences = self.tokenizer.texts_to_sequences(reviews)
        padded = pad_sequences(sequences, maxlen=self.max_sequence_length)
        predictions = self.model.predict(padded)
        pred_labels = [1 if p >= 0.5 else 0 for p in predictions]
        return self.label_encoder.inverse_transform(pred_labels)

# Load and prepare the data
def load_data():
    # Load your Train.csv file
    data = pd.read_csv('/content/Train.csv')


    texts = data['text'].values  # Replace 'text' with your actual text column name
    labels = data['label'].values  # Replace 'sentiment' with your actual label column name

    return texts, labels

# Main execution
if __name__ == "__main__":
    # Load the data
    texts, labels = load_data()

    # Create an instance of the SentimentAnalyzer
    analyzer = SentimentAnalyzer()

    # Train the model
    print("Training the model...")
    analyzer.fit(texts, labels, epochs=5)

    # Test with new reviews
    new_reviews = [
        "The movie was fantastic and I loved it!",
        "It was the worst film I have ever seen."
    ]

    # Make predictions
    predicted_labels = analyzer.predict_sentiment(new_reviews)

    # Print results
    for review, label in zip(new_reviews, predicted_labels):
        print(f"Review: {review} | Predicted Sentiment: {label}")
